"""
Baseline Comparison: Keyword-only vs LLM-RAG
Compares templated answers (no LLM) with LLM-generated answers
"""

import json
from retrieval import RecipeRetriever

def generate_baseline_answer(query, retrieved_recipes):
    """
    Generate baseline templated answer without LLM
    Simple template-based response using retrieved recipe titles

    Args:
        query: User query
        retrieved_recipes: List of retrieved recipes

    Returns:
        Templated answer string
    """
    if not retrieved_recipes:
        return "No recipes found matching your query."

    # Simple template based on number of recipes
    recipe_titles = [r['title'].title() for r in retrieved_recipes[:3]]

    if len(recipe_titles) == 1:
        answer = f"Based on your search, I found: {recipe_titles[0]}."
    elif len(recipe_titles) == 2:
        answer = f"Based on your search, I found: {recipe_titles[0]} and {recipe_titles[1]}."
    else:
        answer = f"Based on your search, I found: {recipe_titles[0]}, {recipe_titles[1]}, and {recipe_titles[2]}."

    # Add basic info from first recipe
    first_recipe = retrieved_recipes[0]
    if 'calories' in first_recipe and first_recipe['calories']:
        answer += f" The first recipe has {first_recipe['calories']:.0f} calories."
    if 'duration' in first_recipe and first_recipe['duration']:
        answer += f" It takes about {first_recipe['duration']:.0f} minutes to prepare."

    return answer

def run_baseline_comparison(test_queries=None):
    """
    Run baseline comparison on sample queries

    Args:
        test_queries: List of test queries (if None, uses default set)
    """
    print("="*70)
    print("Baseline vs LLM-RAG Comparison")
    print("="*70)

    # Load LLM-RAG results
    with open('../evaluation/test_results.json', 'r') as f:
        llm_rag_results = json.load(f)

    # Initialize retriever for baseline
    retriever = RecipeRetriever()

    # Use first 5 queries for demonstration
    comparison_results = []

    for i, llm_result in enumerate(llm_rag_results[:5], 1):
        query = llm_result['query']
        retrieved_recipes = llm_result['retrieved_recipes']
        llm_answer = llm_result['answer']

        # Generate baseline answer
        baseline_answer = generate_baseline_answer(query, retrieved_recipes)

        print(f"\n{'='*70}")
        print(f"Query {i}: {query}")
        print(f"{'='*70}")

        print(f"\nðŸ“‹ BASELINE (Templated, No LLM):")
        print(f"{'-'*70}")
        print(baseline_answer)

        print(f"\nðŸ¤– LLM-RAG (Claude-generated):")
        print(f"{'-'*70}")
        print(llm_answer)

        print(f"\n{'='*70}")

        comparison_results.append({
            'query': query,
            'baseline_answer': baseline_answer,
            'llm_rag_answer': llm_answer,
            'retrieved_recipes': [r['title'] for r in retrieved_recipes[:3]]
        })

    # Save comparison (JSON)
    output_file_json = '../evaluation/baseline_comparison.json'
    with open(output_file_json, 'w') as f:
        json.dump(comparison_results, f, indent=2)

    print(f"\nâœ“ Comparison saved to: {output_file_json}")

    # Save comparison (TXT for easy reading)
    output_file_txt = '../evaluation/baseline_comparison.txt'
    with open(output_file_txt, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("Baseline vs LLM-RAG Comparison\n")
        f.write("="*70 + "\n\n")

        for i, result in enumerate(comparison_results, 1):
            f.write("="*70 + "\n")
            f.write(f"Query {i}: {result['query']}\n")
            f.write("="*70 + "\n\n")

            f.write("Retrieved Recipes:\n")
            for j, title in enumerate(result['retrieved_recipes'], 1):
                f.write(f"  {j}. {title}\n")
            f.write("\n")

            f.write("ðŸ“‹ BASELINE (Templated, No LLM):\n")
            f.write("-"*70 + "\n")
            f.write(result['baseline_answer'] + "\n\n")

            f.write("ðŸ¤– LLM-RAG (Claude-generated):\n")
            f.write("-"*70 + "\n")
            f.write(result['llm_rag_answer'] + "\n\n")

        f.write("\n" + "="*70 + "\n")
        f.write("Analysis: Baseline vs LLM-RAG\n")
        f.write("="*70 + "\n\n")

        f.write("ðŸ“‹ Baseline (Template-only):\n")
        f.write("   Pros:\n")
        f.write("   + Fast and deterministic\n")
        f.write("   + No API costs\n")
        f.write("   + Predictable output format\n\n")
        f.write("   Cons:\n")
        f.write("   - Generic, repetitive responses\n")
        f.write("   - Cannot understand context or nuance\n")
        f.write("   - Limited to recipe titles only\n")
        f.write("   - No reasoning or explanation\n\n")

        f.write("ðŸ¤– LLM-RAG:\n")
        f.write("   Pros:\n")
        f.write("   + Natural, conversational answers\n")
        f.write("   + Understands query intent\n")
        f.write("   + Provides relevant details (calories, health, etc.)\n")
        f.write("   + Can explain reasoning\n\n")
        f.write("   Cons:\n")
        f.write("   - Requires API calls (cost ~$0.02 for 20 queries)\n")
        f.write("   - Slight latency (1-2 seconds per query)\n")
        f.write("   - Non-deterministic output\n\n")

        f.write("="*70 + "\n")
        f.write("Conclusion:\n")
        f.write("="*70 + "\n")
        f.write("LLM-RAG provides significantly better answer quality by:\n")
        f.write("â€¢ Understanding query intent and context\n")
        f.write("â€¢ Generating natural, helpful responses\n")
        f.write("â€¢ Providing relevant details beyond just recipe names\n")
        f.write("â€¢ Explaining why recipes are recommended\n\n")
        f.write("The small API cost (~$0.001 per query) is worthwhile for the\n")
        f.write("dramatically improved user experience.\n")
        f.write("="*70 + "\n")

    print(f"âœ“ Text version saved to: {output_file_txt}")

    # Print analysis
    print("\n" + "="*70)
    print("Analysis: Baseline vs LLM-RAG")
    print("="*70)

    print("\nðŸ“‹ Baseline (Template-only):")
    print("   Pros:")
    print("   + Fast and deterministic")
    print("   + No API costs")
    print("   + Predictable output format")
    print("\n   Cons:")
    print("   - Generic, repetitive responses")
    print("   - Cannot understand context or nuance")
    print("   - Limited to recipe titles only")
    print("   - No reasoning or explanation")

    print("\nðŸ¤– LLM-RAG:")
    print("   Pros:")
    print("   + Natural, conversational answers")
    print("   + Understands query intent")
    print("   + Provides relevant details (calories, health, etc.)")
    print("   + Can explain reasoning")
    print("\n   Cons:")
    print("   - Requires API calls (cost ~$0.02 for 20 queries)")
    print("   - Slight latency (1-2 seconds per query)")
    print("   - Non-deterministic output")

    print("\n" + "="*70)
    print("Conclusion:")
    print("="*70)
    print("LLM-RAG provides significantly better answer quality by:")
    print("â€¢ Understanding query intent and context")
    print("â€¢ Generating natural, helpful responses")
    print("â€¢ Providing relevant details beyond just recipe names")
    print("â€¢ Explaining why recipes are recommended")
    print("\nThe small API cost (~$0.001 per query) is worthwhile for the")
    print("dramatically improved user experience.")
    print("="*70)

    return comparison_results

if __name__ == "__main__":
    results = run_baseline_comparison()
    print("\nâœ“ Baseline comparison complete!")
