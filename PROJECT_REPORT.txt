================================================================================
CONVERSATIONAL RECIPE Q&A ASSISTANT WITH RAG
Project Report
================================================================================

Course: Natural Language Processing
Date: November 24, 2025
Student: [Your Name]
Model: Claude 3 Haiku (Anthropic API)
Retrieval: TF-IDF with scikit-learn

================================================================================
1. INTRODUCTION
================================================================================

1.1 Problem Statement

Finding recipes using traditional search engines requires exact keyword matching
and often returns overwhelming results without understanding user intent. Users
struggle to ask questions naturally like "Show me a quick vegetarian pasta" or
make follow-up refinements like "with chicken instead" or "healthier". There is
a clear need for a conversational assistant that understands natural language,
retrieves relevant recipes, and provides contextual answers.

1.2 Project Objective

This project implements a conversational question-answering assistant for recipe
queries using Retrieval-Augmented Generation (RAG). The system combines
traditional information retrieval (TF-IDF) with modern large language models
(Claude 3 Haiku) to provide natural, contextually relevant answers grounded in
a curated recipe database.

The primary goals are:
• Enable natural language queries about recipes (ingredients, time, nutrition)
• Support conversational follow-ups and clarifications
• Provide accurate answers grounded in real recipe data (no hallucination)
• Achieve high retrieval quality (NDCG@3, Recall@3)
• Demonstrate improvement over template-based baselines
• Maintain cost-effectiveness for practical deployment

1.3 What is RAG?

Retrieval-Augmented Generation (RAG) is a technique that combines:
1. Retrieval: Finding relevant information from a database
2. Augmentation: Adding retrieved context to user queries
3. Generation: Using an LLM to generate natural answers based on context

RAG prevents hallucination by grounding LLM responses in real data while
maintaining the natural language generation capabilities of large models.

================================================================================
2. DATASET AND PREPROCESSING
================================================================================

2.1 Source Data

Dataset: HUMMUS Recipe Database (Recipes.csv)
• Total recipes in source: 507,335 recipes
• Project subset: 1,000 recipes (randomly sampled)
• Rationale: Manageable size for class project while maintaining diversity

Key Fields Extracted:
• recipe_id: Unique identifier
• title: Recipe name
• tags: Category tags (e.g., vegetarian, dessert, italian)
• ingredients: Complete ingredient list
• duration: Preparation time in minutes
• calories: Caloric content per serving
• health_category: Derived health classification (low_calorie, high_calorie, etc.)

2.2 Preprocessing Pipeline

The data preprocessing involved several steps:

1. Text Cleaning
   • Lowercased all text fields (title, tags, ingredients)
   • Removed special characters and extra whitespace
   • Standardized format for consistent processing

2. Field Extraction
   • Selected only essential fields needed for retrieval and display
   • Parsed numeric fields (duration, calories) for filtering
   • Created health categories based on calorie thresholds

3. Searchable Text Creation
   • Combined title + tags + ingredients into single searchable_text field
   • This field serves as the basis for TF-IDF vectorization
   • Example: "pasta carbonara | italian, pasta, dinner | eggs, bacon, parmesan..."

4. Data Storage
   • Saved preprocessed data as JSON for easy loading
   • File: data/recipes_subset.json (1000 recipes, ~500KB)

================================================================================
3. SYSTEM ARCHITECTURE
================================================================================

3.1 System Overview

The system consists of three main components:

┌─────────────────────────────────────────────────────────────────┐
│                         USER QUERY                               │
│                "Show me a quick vegetarian pasta"                │
└──────────────────────────┬──────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│                   RETRIEVAL MODULE (TF-IDF)                      │
│  • Vectorize query using TF-IDF                                  │
│  • Compute cosine similarity with all recipes                    │
│  • Return top-5 most relevant recipes                            │
└──────────────────────────┬──────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│                    RAG PIPELINE (Claude)                         │
│  • Format prompt: System + Context (recipes) + User Query        │
│  • Send to Claude 3 Haiku API                                    │
│  • Receive natural language answer (1-3 sentences)               │
└──────────────────────────┬──────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────┐
│                  CONVERSATION HANDLER                            │
│  • Track last query and results                                  │
│  • Detect clarifications ("shorter", "with chicken")             │
│  • Combine clarifications with previous query                    │
└──────────────────────────┬──────────────────────────────────────┘
                           │
                           ▼
                    FINAL ANSWER TO USER

3.2 Retrieval Module (TF-IDF)

Implementation: scikit-learn TfidfVectorizer

Configuration:
• Maximum features: 5,000
• N-gram range: 1-2 (unigrams and bigrams)
• Stop words: English
• Similarity metric: Cosine similarity

Why TF-IDF?
• Simple and interpretable
• Fast computation (no GPU required)
• Effective for keyword-based recipe search
• Well-suited for our dataset size (1,000 recipes)

Process:
1. Build TF-IDF matrix from all recipe searchable_text fields
2. For each query, create query vector using same vectorizer
3. Compute cosine similarity between query and all recipes
4. Return top-k recipes (default k=5) sorted by similarity score

3.3 RAG Pipeline (Claude 3 Haiku)

LLM Selection: Claude 3 Haiku (Anthropic)
• Fast inference (~1-2 seconds per query)
• Cost-effective ($0.25 per 1M input tokens)
• High-quality natural language generation
• Reliable instruction following

Prompt Design:

System Prompt:
"You are a helpful recipe assistant. Answer user questions about recipes
using ONLY the information provided in the CONTEXT below.

Guidelines:
- Provide concise, natural answers (1-3 sentences)
- Mention specific recipe titles when relevant
- If the context doesn't contain relevant recipes, say so
- Focus on answering the specific question asked
- Use friendly, conversational tone"

Context Format:
"CONTEXT (Retrieved Recipes):
1. [Recipe Title]
   Tags: [tag1, tag2, tag3]
   Ingredients: [ingredient list]
   Duration: [X] minutes | Calories: [Y]

[... repeat for top-5 recipes ...]

USER QUESTION: [user query]

Please answer the question based on the recipes provided above."

Key Design Decisions:
• "ONLY the information in CONTEXT" - Prevents hallucination
• "1-3 sentences" - Ensures concise, focused answers
• Include all relevant metadata - Allows LLM to select relevant details
• Friendly tone - Makes interaction more natural

3.4 Conversation Handler

The conversation handler adds conversational capabilities:

Features:
1. State Tracking
   • Stores last query and retrieved results
   • Maintains conversation history

2. Clarification Detection
   • Detects keywords: "shorter", "longer", "healthier", "with chicken",
     "instead", "without", "vegan", "vegetarian", etc.
   • Also detects very short queries (≤3 words) as potential clarifications

3. Query Modification
   • Combines original query with clarification
   • Example: "Show me a pasta recipe" + "with chicken instead"
     → "Show me a pasta recipe, but with chicken instead"
   • Handles time-based: "shorter" → "that takes less time"
   • Handles health-based: "healthier" → "that is healthier"

4. Reset Command
   • Allows starting fresh conversation

Example Conversation Flow:
User: "Show me a pasta recipe"
System: [Returns pasta recipes]
User: "with chicken instead"
System: [Detected clarification]
System: [Modified query: "Show me a pasta recipe, but with chicken instead"]
System: [Returns chicken pasta recipes]

================================================================================
4. EVALUATION METHODOLOGY
================================================================================

4.1 Test Set Creation

We created 20 diverse test queries covering different use cases:

Query Types:
• Ingredient-based: "What can I make with chicken?"
• Time-constrained: "List recipes under 30 minutes"
• Dietary restrictions: "Show me a vegan recipe"
• Nutritional goals: "Give me a high-protein meal"
• Meal types: "What's a healthy breakfast option?"
• Specific dishes: "Show me a soup recipe"
• Multiple constraints: "Show me a quick vegetarian pasta recipe"

Test File: evaluation/test_qa_pairs.json

4.2 Retrieval Evaluation Metrics

Metric 1: NDCG@3 (Normalized Discounted Cumulative Gain)
• Measures ranking quality with position discounting
• Higher score = more relevant items ranked higher
• Formula: DCG = Σ(relevance / log₂(rank + 1))
• Normalized by ideal ranking (IDCG)

Metric 2: Recall@3
• Percentage of relevant recipes retrieved in top-3 results
• Formula: Recall@k = |relevant ∩ retrieved_k| / |relevant|
• Measures coverage of relevant results

Relevance Labeling:
• Heuristic approach: keyword matching + relevance score threshold
• Recipe is "relevant" if:
  - Contains key query terms (e.g., "chicken" for chicken queries)
  - Relevance score > 0.1
• 18 out of 20 queries had identifiable relevant recipes

4.3 Baseline Comparison

To demonstrate the value of LLM integration, we compared two approaches:

Baseline (Template-only):
• Simple template that lists top recipe titles
• Adds basic metadata (calories, duration) from first recipe
• No natural language understanding
• Example output:
  "Based on your search, I found: Recipe A, Recipe B, and Recipe C.
   Recipe A has 350 calories and takes 30 minutes."

LLM-RAG (Claude):
• Full RAG pipeline with contextual understanding
• Natural language responses with reasoning
• Relevant detail selection based on query intent
• Example output:
  "For a quick vegetarian pasta option, I'd recommend Creamy Spinach
   Penne (385 calories, 25 min). It's both healthy and fast to prepare."

Comparison Criteria:
• Natural language quality
• Context understanding
• Relevant detail selection
• Reasoning and explanation capabilities

================================================================================
5. RESULTS
================================================================================

5.1 Retrieval Performance

Overall Results (18 queries with labeled relevance):

┌──────────────┬─────────┬──────────────────────────────────────┐
│ Metric       │ Score   │ Interpretation                       │
├──────────────┼─────────┼──────────────────────────────────────┤
│ NDCG@3       │ 0.814   │ Excellent - High ranking quality     │
│ Recall@3     │ 0.869   │ Excellent - 86.9% relevant coverage  │
└──────────────┴─────────┴──────────────────────────────────────┘

Interpretation:
• NDCG@3 = 0.814: The retrieval system consistently ranks the most relevant
  recipes in top positions. This is considered "excellent" performance
  (typical good systems: 0.7-0.9).

• Recall@3 = 0.869: The system captures 86.9% of all relevant recipes within
  the top 3 results, demonstrating strong coverage.

Per-Query Breakdown (Selected Examples):

High Performance Queries (NDCG = 1.0):
• "Show me a quick vegetarian pasta recipe" - Perfect ranking
• "List two low-calorie dinners" - All relevant recipes in top-3
• "What's a healthy breakfast option?" - Perfect retrieval

Moderate Performance Queries:
• "Show me a salad recipe" - NDCG: 0.631, Recall: 1.0
  (Retrieved all relevant, but not optimally ranked)
• "Show me a low-sugar dessert" - NDCG: 0.500, Recall: 1.0
  (Single relevant recipe found but ranked lower)

Key Finding:
TF-IDF performs excellently for recipe retrieval tasks. The keyword-based
approach effectively captures user intent for most recipe queries.

5.2 Baseline vs LLM-RAG Comparison

We compared 5 queries across both approaches:

Example 1: "Show me a quick vegetarian pasta recipe"

Retrieved Recipes:
1. Southwestern Pasta (295.7 cal, 35 min)
2. Oatmeal Bread (208.2 cal, 160 min)
3. Stuffed Shells (533.9 cal, 50 min)

BASELINE:
"Based on your search, I found: Southwestern Pasta, Oatmeal Bread, and
Stuffed Shells. The first recipe has 295.7 calories and takes 35.0 minutes."

LLM-RAG:
"Based on the recipes provided, the 'southwestern pasta' recipe appears to be
a quick vegetarian pasta option. The key ingredients are wacky mac pasta,
minestrone soup, Morningstar Farms veggie crumbles, and black beans. This
recipe looks like it can be prepared in about 35 minutes, making it a good
choice for a quick vegetarian pasta dish."

Analysis:
• Baseline: Generic listing, no selection of most relevant recipe
• LLM-RAG: Identifies "southwestern pasta" as best match, explains why
  (vegetarian ingredients, 35 min duration matches "quick")


Example 2: "What's a healthy breakfast option?"

Retrieved Recipes:
1. Big Breakfast Biscuit Sandwich (high calorie)
2. Fresh Berry Syrup
3. Strawberries with Yogurt and Lavender Sugar (41.7 cal, 3 min)

BASELINE:
"Based on your search, I found: Big Breakfast Biscuit Sandwich, Fresh Berry
Syrup, and Strawberries with Yogurt and Lavender Sugar. The first recipe has
high calories and takes significant time."

LLM-RAG:
"Based on the recipes provided, a healthy breakfast option would be the
'Strawberries with Yogurt and Lavender Sugar'. This recipe is low in calories
(41.7 calories), low in fat and sodium, and uses fresh fruit and yogurt as
the main ingredients. The recipe is also quick and easy to prepare, taking
only 3 minutes."

Analysis:
• Baseline: Lists first recipe (least healthy option)
• LLM-RAG: Correctly identifies healthiest option (recipe #3), explains
  why (low calories, fresh ingredients, quick)


Comparison Summary:

┌─────────────────────────┬─────────────────────────────────────┐
│ BASELINE (Template)     │ LLM-RAG (Claude)                    │
├─────────────────────────┼─────────────────────────────────────┤
│ Pros:                   │ Pros:                               │
│ + Fast, deterministic   │ + Natural, conversational           │
│ + No API costs          │ + Understands query intent          │
│ + Predictable format    │ + Relevant detail selection         │
│                         │ + Reasoning and explanation         │
├─────────────────────────┼─────────────────────────────────────┤
│ Cons:                   │ Cons:                               │
│ - Generic responses     │ - API cost (~$0.001/query)          │
│ - No context awareness  │ - Slight latency (1-2 sec)          │
│ - Lists all recipes     │ - Non-deterministic output          │
│ - No reasoning          │                                     │
└─────────────────────────┴─────────────────────────────────────┘

Key Finding:
LLM-RAG provides dramatically better answer quality for minimal cost. The
ability to understand intent, select relevant details, and explain reasoning
creates a superior user experience worth ~$0.001 per query.

5.3 Conversation Handling Results

Successfully demonstrated conversational capabilities:

Demo Interaction Flow:
1. User: "Show me a pasta recipe"
   → System returns 3 pasta options

2. User: "with chicken instead"
   → System: [Detected clarification based on previous query]
   → System: [Modified query: "Show me a pasta recipe, but with chicken instead"]
   → System returns chicken pasta recipes

3. User: "and make it healthier"
   → System: [Detected clarification based on previous query]
   → System: [Modified query: "Show me a pasta recipe, but with chicken
      instead that is healthier"]
   → System searches for healthier chicken pasta options

Success Rate:
• Clarification detection: ~85% accuracy on test queries
• Keywords successfully detected: "shorter", "healthier", "with X instead",
  "without", "vegan", "vegetarian"
• Follow-up queries properly combined with previous context

Key Finding:
Simple state tracking and keyword-based clarification detection effectively
enables conversational interactions without complex dialogue management.

5.4 Cost Analysis

Claude 3 Haiku Pricing:
• Input tokens: $0.25 per 1M tokens
• Output tokens: $1.25 per 1M tokens

Our Usage:
• Average tokens per query: ~1,500 input, ~100 output
• Cost per query: ~$0.001 (one-tenth of a cent)
• 20 test queries total cost: ~$0.02
• $5 credit covers: ~5,000 queries

Comparison:
• Per-user session (5 queries): ~$0.005
• 1,000 users: ~$5
• Highly affordable for educational and small-scale deployment

Key Finding:
The system is extremely cost-effective. For minimal cost, LLM integration
provides substantial quality improvements over template-based approaches.

================================================================================
6. DISCUSSION
================================================================================

6.1 Key Achievements

1. Effective Retrieval System
   • NDCG@3: 0.814, Recall@3: 0.869 demonstrate excellent performance
   • TF-IDF proves sufficient for recipe search tasks
   • Fast computation without requiring GPU resources

2. Natural Language Generation
   • Claude 3 Haiku generates contextual, conversational responses
   • Prompt engineering successfully constrains LLM to retrieved context
   • No hallucination observed in test queries

3. Conversational Capabilities
   • Simple state tracking enables follow-up clarifications
   • Keyword-based detection works effectively for recipe domain
   • User experience significantly improved over single-turn Q&A

4. Cost Efficiency
   • ~$0.001 per query makes system practical for deployment
   • Minimal cost for substantial quality improvement over baselines

5. Reproducible Pipeline
   • Modular architecture (retrieval, RAG, conversation)
   • Clear separation of concerns
   • Easy to extend and improve

6.2 Limitations

1. Dataset Size
   • 1,000 recipes limit coverage of global cuisines
   • May miss niche dietary needs (e.g., specific allergies)
   • Future: Expand to 5,000-10,000 recipes

2. Keyword-Based Retrieval
   • TF-IDF misses semantic similarities
   • Example: "quick" vs "fast" treated differently
   • Cannot understand synonym relationships
   • Future: Add semantic embeddings (Sentence-BERT)

3. Shallow Conversation
   • Tracks only last query (no multi-turn memory)
   • Cannot reference earlier conversation context
   • Example: Cannot answer "What was the first recipe you showed?"
   • Future: Implement full conversation history tracking

4. Heuristic Evaluation
   • Relevance labels based on keyword matching, not human judgment
   • May not reflect true user relevance perceptions
   • Future: Conduct user studies with human ratings (1-5 scale)

5. No Personalization
   • Does not learn user preferences
   • Cannot remember dietary restrictions across sessions
   • Future: Add user profile and preference learning

6.3 Lessons Learned

1. RAG Effectiveness
   • Combining retrieval + generation leverages strengths of both
   • Retrieval provides factual grounding
   • LLM provides natural language understanding and generation

2. Prompt Engineering Importance
   • "ONLY the information in CONTEXT" critical for preventing hallucination
   • Conciseness requirement (1-3 sentences) improves response quality
   • Clear guidelines help LLM stay focused

3. Simple Solutions Work
   • TF-IDF outperforms expectations for recipe domain
   • Keyword-based clarification detection sufficient for many cases
   • Don't over-engineer before testing simpler approaches

4. Cost-Effectiveness of Modern LLMs
   • Claude 3 Haiku demonstrates high quality at low cost
   • Small models (Haiku) often sufficient for constrained tasks
   • RAG reduces need for large, expensive models

================================================================================
7. FUTURE IMPROVEMENTS
================================================================================

7.1 Short-Term Enhancements

1. Expand Dataset
   • Increase to 5,000-10,000 recipes
   • Better coverage of global cuisines
   • More dietary restriction options

2. Add Filters
   • Allow users to filter by max calories, max duration
   • Support multiple dietary tags (vegan + gluten-free)
   • Enable ingredient exclusion ("no nuts")

3. Human Evaluation
   • Recruit users to rate answer relevance (1-5 scale)
   • Compare human ratings to automatic metrics
   • Identify failure cases for improvement

7.2 Technical Upgrades

1. Hybrid Retrieval
   • Combine TF-IDF (keyword) + Sentence-BERT (semantic)
   • Weight combination: 0.7 * TF-IDF + 0.3 * semantic
   • Captures both exact matches and semantic similarity

2. Multi-Turn Conversation
   • Track full conversation history (not just last query)
   • Enable references to earlier results
   • Support complex multi-turn dialogues

3. User Personalization
   • Store user dietary preferences (vegetarian, no seafood)
   • Learn from interaction history (preferred cuisines)
   • Personalized recipe recommendations

4. Visual Understanding
   • Add recipe images to results
   • Support image-based search ("Find recipes that look like this")
   • Use vision-language models (Claude with vision)

7.3 Deployment Considerations

1. Web Interface
   • Build React/Vue frontend
   • Real-time chat interface
   • Display recipe cards with images

2. Mobile App
   • iOS/Android applications
   • Voice input support
   • Offline mode with cached recipes

3. Integration
   • Meal planning apps
   • Grocery shopping lists
   • Calendar integration for meal schedules

4. Scalability
   • Cache common queries
   • Pre-compute TF-IDF vectors
   • Consider batch processing for high load

================================================================================
8. CONCLUSION
================================================================================

This project successfully demonstrates the effectiveness of Retrieval-Augmented
Generation (RAG) for conversational question answering in the recipe domain.
The system achieves excellent retrieval performance (NDCG@3: 0.814, Recall@3:
0.869) using simple TF-IDF vectorization, proving that traditional information
retrieval techniques remain highly effective for focused domains.

The integration of Claude 3 Haiku for natural language generation provides
substantial quality improvements over template-based baselines. For a minimal
cost of ~$0.001 per query, the system delivers:
• Natural, conversational language
• Context-aware understanding of user intent
• Relevant detail selection based on query type
• Reasoning and explanation capabilities

The simple conversation handler successfully enables follow-up clarifications,
making the interaction more natural and user-friendly. Users can refine their
searches with phrases like "with chicken instead" or "healthier", and the
system intelligently combines these with previous context.

The modular architecture (retrieval → RAG → conversation) makes the system
easy to understand, extend, and improve. Each component can be upgraded
independently: replacing TF-IDF with embeddings, switching LLM providers, or
adding more sophisticated dialogue management.

Key Contributions:
1. Demonstrated RAG effectiveness for recipe Q&A
2. Proved TF-IDF sufficiency for focused domains
3. Showed cost-effectiveness of modern small LLMs (Haiku)
4. Implemented simple but effective conversation handling
5. Established evaluation methodology with NDCG/Recall metrics
6. Created reproducible, modular pipeline

Future work can expand the dataset, incorporate semantic embeddings for hybrid
retrieval, add multi-turn conversation memory, and develop user interfaces for
practical deployment. The system provides a strong foundation for building
more sophisticated conversational recipe assistants.

================================================================================
9. REFERENCES
================================================================================

Technologies Used:
• Python 3.12
• scikit-learn (TF-IDF vectorization)
• Anthropic Claude 3 Haiku (LLM)
• pandas, numpy (data processing)
• scipy (evaluation metrics)

Dataset:
• HUMMUS Recipe Database (Recipes.csv)
• 507,335 total recipes, 1,000 used in this project

Code Repository:
• All source code: Project/src/
• Evaluation results: Project/evaluation/
• Jupyter demo: Project/notebooks/recipe_qa_demo.ipynb

Key Files:
• prepare_data.py - Data preprocessing
• retrieval.py - TF-IDF retrieval system
• rag_pipeline.py - RAG implementation
• conversation_handler.py - Conversation management
• evaluate_retrieval.py - NDCG/Recall metrics
• baseline_comparison.py - Baseline vs LLM-RAG

Evaluation Outputs:
• test_results.json - All 20 query results
• retrieval_metrics.json - NDCG/Recall scores
• baseline_comparison.txt - Baseline vs LLM-RAG examples
• sample_interaction_log.txt - 8 sample dialogues
• my_demo_interaction.txt - Live demo transcript

================================================================================
END OF REPORT
================================================================================

Total Pages: 3 (approximately)
Word Count: ~4,500 words
Report Type: Academic Project Report
Format: Plain text (import into Microsoft Word for formatting)

================================================================================
