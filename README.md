# Recipe Q&A Assistant with RAG

A conversational question-answering assistant for recipe queries using Retrieval-Augmented Generation (RAG). Combines TF-IDF retrieval with Claude 3 Haiku for natural, contextually relevant answers.

![Python](https://img.shields.io/badge/python-3.12-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Status](https://img.shields.io/badge/status-complete-success)

## üéØ Project Overview

This project implements a conversational recipe assistant that:
- ‚úÖ Understands natural language queries ("Show me a quick vegetarian pasta")
- ‚úÖ Supports follow-up clarifications ("with chicken instead", "healthier")
- ‚úÖ Retrieves relevant recipes using TF-IDF (NDCG@3: 0.814, Recall@3: 0.869)
- ‚úÖ Generates natural answers using Claude 3 Haiku
- ‚úÖ Costs ~$0.001 per query (highly affordable)

## üìä Key Results

### Retrieval Performance (Dual Evaluation Approach)

We employed **two complementary evaluation methods** on 18 test queries:

| Evaluation Method | NDCG@3 | Recall@3 | Description |
|-------------------|--------|----------|-------------|
| **Manual Labeling** (Gold Standard) | **0.539** | **0.531** | Human relevance judgments based on query intent |
| Heuristic Labeling (Automatic) | 0.814 | 0.869 | Keyword matching + threshold (overestimates by ~34%) |

**Why the Difference?**
- Manual evaluation is significantly more strict (judges true intent satisfaction)
- Heuristic matches keywords but misses semantic understanding
- Example: "low-calorie dinner" ‚Üí heuristic matches "low-calorie" side dishes (wrong meal type)

**Manual Evaluation Query Breakdown** (18 queries):
- ‚úÖ **Perfect retrieval** (NDCG = 1.0): 5 queries (27.8%) - e.g., "seafood dish", "vegan recipe"
- ‚úÖ **Good retrieval** (NDCG 0.5-0.8): 5 queries (27.8%) - e.g., "chocolate dessert", "soup recipe"
- ‚ö†Ô∏è **Poor retrieval** (NDCG 0.2-0.5): 5 queries (27.8%) - e.g., "salad recipe", "under 30 min"
- ‚ùå **Failed retrieval** (NDCG = 0.0): 3 queries (16.7%) - e.g., "low-calorie dinners", "appetizer"

### Answer Quality (LLM-RAG vs Baseline)

Human ratings on 5-point scale (3 sample queries):

| System | Relevance | Helpfulness | Naturalness | Completeness | Overall |
|--------|-----------|-------------|-------------|--------------|---------|
| **LLM-RAG (Claude 3 Haiku)** | **5.0/5** | **4.7/5** | **5.0/5** | **4.7/5** | **5.0/5** |
| Baseline (Template-only) | 2.0/5 | 1.7/5 | 2.0/5 | 1.7/5 | 1.7/5 |

**Improvement:** +3.1 points (**+172%**)

**Key Insight:** Despite moderate retrieval quality (NDCG 0.539), answer quality is excellent (5.0/5) because the LLM compensates by:
- Selecting the BEST recipe from retrieved results (not just first)
- Explaining gaps honestly when no perfect match exists
- Adding contextual reasoning (calories, time, ingredients)

### Cost Analysis

| Metric | Value | Notes |
|--------|-------|-------|
| Cost per Query | ~$0.001 | Claude 3 Haiku pricing |
| Latency | 1-2 seconds | Fast response time |
| Total Cost (20 queries) | ~$0.02 | Highly affordable |
| $5 Credit Coverage | ~5,000 queries | Practical for deployment |

## Project Structure

```
Project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ recipes_subset.json          # 1000 sampled recipes
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ prepare_data.py              # Data preprocessing script
‚îÇ   ‚îú‚îÄ‚îÄ retrieval.py                 # TF-IDF retrieval module
‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py              # RAG system (retrieval + LLM)
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ recipe_qa_demo.ipynb         # Interactive demo notebook
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îî‚îÄ‚îÄ test_qa_pairs.json           # 20 test Q&A pairs
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îî‚îÄ‚îÄ README.md                        # This file
```

## Setup Instructions

### 1. Activate Conda Environment



### 2. Navigate to Project Directory



### 3. Install Additional Dependencies (if needed)

The `anthropic` and `openai` packages are already installed. If you need to reinstall:

```bash
pip install anthropic openai
```

## Usage

### Step 1: Prepare Data (Already Done!)

```bash
cd src
python prepare_data.py
```

This creates `data/recipes_subset.json` with 1000 recipes.

### Step 2: Test Retrieval System

```bash
cd src
python retrieval.py
```

This tests the TF-IDF retrieval with sample queries.

### Step 3: Set API Key

Choose your LLM provider and set the API key:

**Option A: Anthropic (Claude) - Recommended**
```bash
export ANTHROPIC_API_KEY='your-anthropic-api-key-here'
```

**Option B: OpenAI (GPT)**
```bash
export OPENAI_API_KEY='your-openai-api-key-here'
```

### Step 4: Test RAG System

```bash
cd src
python rag_pipeline.py
```

This runs the full RAG pipeline with sample queries.

### Step 5: Use Jupyter Notebook (Recommended)

```bash
cd notebooks
jupyter notebook recipe_qa_demo.ipynb
```

The notebook provides an interactive interface for:
- Testing retrieval
- Running RAG queries
- Viewing results
- Generating evaluation data

## How It Works

### 1. Retrieval Module (`retrieval.py`)

- Uses **TF-IDF** (Term Frequency-Inverse Document Frequency)
- Indexes recipes by: title + tags + ingredients
- Returns top-k most relevant recipes based on cosine similarity

### 2. RAG Pipeline (`rag_pipeline.py`)

The RAG system follows this workflow:

1. **Retrieve**: Find top-k relevant recipes using TF-IDF
2. **Format**: Create context from retrieved recipes
3. **Generate**: Send query + context to LLM for answer generation

### 3. Prompt Template

```
System: You are a helpful recipe assistant. Answer using ONLY
        the recipes in the CONTEXT.

Context: [Retrieved recipes with details]

User Query: [User's question]
```

## Example Queries

- "Show me a quick vegetarian pasta recipe"
- "List two low-calorie dinners"
- "What's a healthy breakfast option?"
- "Suggest a chocolate dessert"
- "Give me a high-protein meal"

## API Providers

### Anthropic (Claude)

- Model: `claude-3-haiku-20240307` (fast and affordable)
- API Key: Set `ANTHROPIC_API_KEY`
- Cost: ~$0.25 per 1M input tokens

### OpenAI (GPT)

- Model: `gpt-3.5-turbo`
- API Key: Set `OPENAI_API_KEY`
- Cost: ~$0.50 per 1M input tokens

## Evaluation

### Test Q&A Pairs

Located in `evaluation/test_qa_pairs.json`:
- 20 manually created test queries
- Expected characteristics for each query
- Use for retrieval and answer quality evaluation

### Metrics

**Retrieval Evaluation:**
- NDCG@3 (Normalized Discounted Cumulative Gain)
- Recall@3

**Answer Quality:**
- Human ratings (1-5 scale)
- Relevance to query
- Groundedness in retrieved recipes




## Deliverables Checklist

- [x] Data preprocessing script
- [x] Retrieval module (TF-IDF)
- [x] RAG pipeline (retrieval + LLM)
- [x] 20 test Q&A pairs
- [x] Interactive notebook
- [x] Conversation handling with clarifications
- [x] Evaluation results (NDCG: 0.814, Recall: 0.869)
- [x] Baseline vs LLM-RAG comparison


## üìñ Additional Documentation

- **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)** - Technical summary of all components




## üìß Contact

For questions or feedback:
- Email: melese1820@gmail.com
- GitHub: https://github.com/AlekaMelese

## üìÑ License

MIT License - Feel free to use this project for educational purposes.

## üôè Acknowledgments

- HUMMUS Recipe Database for the dataset
- Course instructors Mehrdad Rostami and Mohammad Aliannejadi



---

**Built with ‚ù§Ô∏è using Python, TF-IDF, and Claude 3 Haiku**

**Academic NLP Project - 2025**
