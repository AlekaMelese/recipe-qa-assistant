Project 1: Conversational Recipe Q&A Assistant with Lightweight RAG
1. Goal and Scope
Build a simple conversational assistant that answers user questions about recipes (ingredients, time,
healthiness) using an LLM plus a small retrieval module. Target use: single-user, short
conversations like:
• “Show me a quick vegetarian pasta.”
The LLM is used for:
• Interpreting user questions and extracting constraints.
• Generating short, natural answers grounded in retrieved recipe data.
2. Dataset
Use a small subset of the HUMMUS (pp_recipes.zip) recipe dataset (e.g., 1–5k recipes):
Key fields:
• recipe_id, title, tags, ingredients, duration, calories, protein, sugars, sodium,
who_score_normalized, health.
Create 10–20 manual Q&A pairs as a test/evaluation set, e.g.
• Q: “List two low-calorie dinners.” → Relevant recipes.
3. System Components
1. Data Preprocessing
o Clean and lowercase text (title, tags, ingredients).
o Keep only a few numeric fields (e.g., calories, duration, who_score_normalized).
2. Retrieval Module (Simple RAG)
o Represent each recipe with a short text: title + tags + ingredients
o Use either:
▪ Keyword search (e.g., TF-IDF / BM25 via a library), or
▪ Sentence embeddings (e.g., a small transformer) with cosine similarity.
3. LLM Question Answering
o LLM input = user question + top-k retrieved recipes (as short summaries).
o LLM output = concise answer (1–3 sentences) and, optionally, recipe titles.
4. Simple Conversation Handling
o Track only the last one user query and result list (no complex state).
o Support clarifications like “shorter”, “with chicken instead”.
4. Development Pipeline
1. Data Setup
o Load and sample a small subset of HUMMUS.
o Preprocess text and keep core numeric fields.
2. Index & Retrieval
o Build a simple index (TF-IDF or embeddings).
o Implement a function: retrieve_recipes (user_query, k=5) → list of recipes.
3. LLM Integration (RAG)
o Design a prompt template:
▪ System: “You are a recipe assistant. Answer only using the recipes in
CONTEXT.”
▪ Context: top-k recipes formatted (title, duration, calories, tags).
▪ User: original question.
o Implement answer_query(query) that calls retrieval then the LLM.
4. Tiny “Conversation” Wrapper
o CLI or notebook cell where the user can type multiple questions in sequence.
5. Evaluation Methodology
1. Retrieval Evaluation (Small & Manual)
o For 10–20 test questions, manually mark which recipes are relevant.
o Compute NDCG@3 and Recall@3 for the retrieval step.
2. Answer Quality (Quick Human Check)
▪ For the same questions, rate answers on: Relevance (1–5)
Compare:
• Baseline: keyword-only retrieval + templated answer (no LLM).
• LLM-RAG: retrieval + LLM-generated answer.
6. Expected Deliverables
• Minimal prototype (notebook or script) with retrieve_recipes and answer_query.
• Short report (2–3 pages) describing: dataset subset, architecture, prompt, and evaluation
results.
• Sample interaction log (5–10 example dialogues) showing typical queries and system
responses.
• Project Presentation (10-15 minutes presentation) explaining the findings